{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling Data From Web PTA.Trunojoyo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebelum memproses suatu data, diperlukan data yang akan diproses. Disini saya akan mengambil data PTA Universitas Trunojoyo Madura. Berikut code program untuk crawling data menggunakan Scrapy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scrapy\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class Url(scrapy.Spider):\n",
    "    name = \"url\"\n",
    "    start_urls = []\n",
    "    \n",
    "    def _init_(self):\n",
    "        url = 'https://pta.trunojoyo.ac.id/c_search/byprod/7/'\n",
    "        for page in range(1,13):\n",
    "            self.start_urls.append(url + str(page))\n",
    "        \n",
    "    def parse(self, response):\n",
    "        for page in range(1,6):\n",
    "            for url in response.css('#content_journal > ul'):\n",
    "                yield {\n",
    "                    'link' : url.css('li:nth-child(1) > div:nth-child(3) > a ::attr(href)').extract()\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "script diatas dijalankan kedalam file bereksistensi \".py\" lalu menjalankan perintah berikut di CMD atau terminal ditempat file ini berada\n",
    "\"scrapy runspider -nama file.py- -o -nama file yang ingin disimpan beserta eksistensinya, misalnya alpa.csv-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodingan diatas untuk mendapatkan link atau url dari abstrak yang akan di crawling datanya, output dari script diatas saya jadikan file csv dengan nama link_pta.csv.\n",
    "untuk cara penggunaan dari scrapy bisa melihat dokumentasi dari scrapynya langsung atau mencari referensi dari youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quote\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        data_csv = pd.read_csv('link_pta.csv').values\n",
    "        start_urls = [ link[0] for link in data_csv ]\n",
    "\n",
    "        for url in start_urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "        \n",
    "    def parse(self, response):\n",
    "        # print(response.url)\n",
    "\n",
    "        for jurnal in response.css('#content_journal > ul > li'):\n",
    "            yield {\n",
    "                'Judul':jurnal.css('div:nth-child(2) > a::text').get(),\n",
    "                'Penulis':jurnal.css('div:nth-child(2) > div:nth-child(2) > span::text').get()[10:],\n",
    "                'Dosbing_1':jurnal.css('div:nth-child(2) > div:nth-child(3) > span::text').get()[21:],\n",
    "                'Dosbing_2':jurnal.css('div:nth-child(2) > div:nth-child(4) > span::text').get()[22:],\n",
    "                'Abstrak_indo':jurnal.css('div:nth-child(4) > div:nth-child(2) > p::text').get(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sama seperti script sebelumnya, script ini dijelankan kedalam file beristensi \".py\" dan hasilnya bisa di simpan dengan perintah di terminal sama seperti yang sebelumnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">hasil dari running script ini saya jadikan file csv dengan nama jurnal.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1b15f5df67a702be8f228f6d0b6061fe0c39795279c81537b9d6008feba9a7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
